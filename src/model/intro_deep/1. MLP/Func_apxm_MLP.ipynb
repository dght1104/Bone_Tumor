{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# (1) import libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import SGD\n\nimport time\nfrom IPython.display import clear_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:55:17.896876Z","iopub.status.idle":"2025-03-25T12:55:17.897302Z","shell.execute_reply":"2025-03-25T12:55:17.897150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (2) data preparation","metadata":{}},{"cell_type":"code","source":"# training ------------------------------\nA = 32j\nx1, x2 = np.mgrid[-3.0:3.0:A, -3.0:3.0:A]\nx1x2 = np.column_stack([x1.flat, x2.flat])\n\nmu = np.array([0.0, 0.0])\ncovariance = np.diag([1, 1])\n\ny = multivariate_normal.pdf(x1x2, mean=mu, cov=covariance)\ny = y.reshape(x1.shape)\n\nx_train = x1x2\ny_train = y.flatten()\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.plot_surface(x1, x2, y)\nplt.title('training data')\nplt.show()\n\n# testing -------------------------------\nA = 10j\n\nx1, x2 = np.mgrid[-3.0:3.0:A, -3.0:3.0:A]\nx1x2 = np.column_stack([x1.flat, x2.flat])\n\nmu = np.array([0.0, 0.0])\ncovariance = np.diag([1, 1])\n\ny = multivariate_normal.pdf(x1x2, mean=mu, cov=covariance)\ny = y.reshape(x1.shape)\n\nx_test = x1x2\ny_test = y.flatten()\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.plot_surface(x1, x2, y)\nplt.title('testing data')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:55:17.898508Z","iopub.status.idle":"2025-03-25T12:55:17.899010Z","shell.execute_reply":"2025-03-25T12:55:17.898784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (3) MLP","metadata":{}},{"cell_type":"code","source":"def gauss_mlp(B=16):\n    # 2 input nodes\n    inp = Input(shape=(2,), name='Input')\n\n    # FC layers\n    x = Dense(units=B, activation='sigmoid', name='fc')(inp)\n\n    # Output\n    gauss_out = Dense(units=1, activation='sigmoid', name='gauss_apxm')(x)\n\n    # define model\n    model = Model(inputs=inp, outputs=gauss_out, name='gauss_mlp')\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:55:17.899988Z","iopub.status.idle":"2025-03-25T12:55:17.900468Z","shell.execute_reply":"2025-03-25T12:55:17.900264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (4) training and evaluating MLP","metadata":{}},{"cell_type":"markdown","source":"### (4.1) Number of training samples (A) (no. of hidden nodes (B) = 16 and no. of epochs (C) = 100)","metadata":{}},{"cell_type":"code","source":"train_error = []\ntest_error = []\ntot_train = 1001\nA_array = []\n\nB = 16\nC = 100\nfor A in np.arange(100, tot_train, 100):\n    # define the model\n    model = gauss_mlp(B=B)\n\n    # compile the model\n    loss = 'mean_squared_error'\n    lr = 0.01\n    model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01)) \n\n    # train the model\n    batch_size = 128\n    epochs = C\n    \n    # items for training\n    sel_indx = np.random.choice(range(1, tot_train), size=A, replace=False)\n    model.fit(x_train[sel_indx], y_train[sel_indx],\n                        validation_data=(x_test, y_test),\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=0)\n\n    train_error.append(model.evaluate(x_train, y_train, verbose=0))\n    test_error.append(model.evaluate(x_test, y_test, verbose=0))\n    A_array.append(A)\n    \n    # plot the relationship between A (training samples) and loss\n    clear_output()\n    plt.plot(A_array, train_error, label='training')\n    plt.plot(A_array, test_error, label='testing')\n    plt.legend()\n    plt.title('Number of training samples (A)')\n    plt.xlabel('#-samples')\n    plt.ylabel('Error')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:55:17.901699Z","iopub.status.idle":"2025-03-25T12:55:17.902194Z","shell.execute_reply":"2025-03-25T12:55:17.901990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### (4.2) Number of hidden nodes (B) (no. of training samples (A) = 800 and no. of epochs (C) = 100)","metadata":{}},{"cell_type":"code","source":"train_error = []\ntest_error = []\nB_array = []\ntot_train = 1001\n    \nA = 800\nB = 8\nC = 100\nwhile B <= 512:\n    # define the model\n    model = gauss_mlp(B=B)\n\n    # compile the model\n    loss = 'mean_squared_error'\n    lr = 0.01\n    model.compile(loss='mean_squared_error', optimizer=SGD(learning_rate=0.01)) \n\n    # train the model\n    batch_size = 128\n    epochs = C\n    \n    # items for training\n    sel_indx = np.random.choice(range(1, tot_train), size=A, replace=False)\n    model.fit(x_train[sel_indx], y_train[sel_indx],\n                        validation_data=(x_test, y_test),\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=0)\n\n    train_error.append(model.evaluate(x_train, y_train, verbose=0))\n    test_error.append(model.evaluate(x_test, y_test, verbose=0))\n    B_array.append(B)\n    \n    # plot the relationship between B (hidden nodes) and loss\n    clear_output()\n    plt.plot(B_array, train_error, label='training')\n    plt.plot(B_array, test_error, label='testing')\n    plt.legend()\n    plt.title('Number of hidden nodes (B)')\n    plt.xlabel('Number of hidden nodes')\n    plt.ylabel('Error')\n    plt.show()\n    \n    B *= 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:55:17.903039Z","iopub.status.idle":"2025-03-25T12:55:17.903511Z","shell.execute_reply":"2025-03-25T12:55:17.903299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### (4.3) Number of epochss (C) (no. of training samples (A) = 800 and no. of hidden nodes (B) = 128)","metadata":{}},{"cell_type":"code","source":"train_error = []\ntest_error = []\ntot_train = 1001\nC_array = []\n\nA = 800\nB = 128\nfor C in np.arange(10, 200, 10):\n    # define the model\n    model = gauss_mlp(B=B)\n\n    # compile the model\n    loss = 'mean_squared_error'\n    lr = 0.01\n    model.compile(loss=loss, optimizer=SGD(lr=lr))\n\n    # train the model\n    batch_size = 128\n    epochs = C\n    \n    # items for training\n    sel_indx = np.random.choice(range(1, tot_train), size=A, replace=False)\n    model.fit(x_train[sel_indx], y_train[sel_indx],\n                        validation_data=(x_test, y_test),\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=0)\n\n    train_error.append(model.evaluate(x_train, y_train, verbose=0))\n    test_error.append(model.evaluate(x_test, y_test, verbose=0))\n    C_array.append(C)\n    \n    # plot the relationship between C (epochs) and loss\n    clear_output()\n    plt.plot(C_array, train_error, label='training')\n    plt.plot(C_array, test_error, label='testing')\n    plt.legend()\n    plt.title('Number of epochs (C)')\n    plt.xlabel('Epochs')\n    plt.ylabel('Error')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T12:55:17.904277Z","iopub.status.idle":"2025-03-25T12:55:17.904584Z","shell.execute_reply":"2025-03-25T12:55:17.904453Z"}},"outputs":[],"execution_count":null}]}